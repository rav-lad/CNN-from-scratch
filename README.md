# CNN from Scratch in NumPy (No Autograd)

**Goal.** Build and train a small convolutional neural network **from scratch** in NumPy.
No autograd. Every forward and backward pass is implemented by hand, including convolution, pooling, batch normalization, and softmax cross entropy.

## ğŸ†• DermaScan - AI Skin Condition Diagnosis

**NEW!** This repository now includes **DermaScan**, a web application that uses the CNN infrastructure to detect skin conditions from images.

ğŸ‘‰ **[See DermaScan Documentation](dermascan/README.md)** | **[View Plan](DERMASCAN_PLAN.md)**

Quick start:
```bash
pip install -r dermascan/requirements.txt
bash dermascan/scripts/run_server.sh
# Open http://localhost:8000
```

## Why this project matters

- Proves **mathematical understanding** of gradients and optimization.
- Demonstrates **clean software design** and testable components.
- Shows **industry readiness** with a reproducible training pipeline and CLI.

## Features

- Layers: `Conv2D`, `MaxPool2D`, `Dense`, `BatchNorm2D`, `Dropout`, common activations.
- Core: manual losses, optimizers (SGD, Momentum, Adam), schedulers, callbacks.
- Utilities: `im2col` and `col2im`, one hot, batching, seed control.
- Datasets: MNIST and CIFAR-10 loaders.
- CLI: `train.py`, `evaluate.py`, `export.py`.
- Tests: strict unit tests with numerical gradient checks.

## Installation

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -U pip
pip install -e ".[dev]"
```

## Quick start

Train a small LeNet on MNIST:

```bash
python -m src.cli.train --config src/configs/mnist_lenet.yaml
```

Evaluate on the test set:

```bash
python -m src.cli.evaluate --config src/configs/mnist_lenet.yaml --weights checkpoints/lenet_mnist_best.npz
```

Export weights and architecture:

```bash
python -m src.cli.export --config src/configs/mnist_lenet.yaml --weights checkpoints/lenet_mnist_best.npz
```

## Project structure

```
cnn-from-scratch/
â”œâ”€ pyproject.toml
â”œâ”€ README.md
â”œâ”€ DERMASCAN_PLAN.md        # ğŸ†• DermaScan project plan
â”œâ”€ data/                    # raw/ and processed/ datasets
â”‚  â””â”€ dermatology/          # ğŸ†• HAM10000 dataset
â”œâ”€ dermascan/               # ğŸ†• Skin diagnosis application
â”‚  â”œâ”€ api/                  # FastAPI server
â”‚  â”œâ”€ inference/            # Model predictions
â”‚  â”œâ”€ preprocessing/        # Image processing
â”‚  â”œâ”€ database/             # Medical conditions info
â”‚  â”œâ”€ configs/              # Model configurations
â”‚  â””â”€ scripts/              # Utilities
â”œâ”€ frontend/                # ğŸ†• Web interface
â”‚  â”œâ”€ static/               # CSS, JS
â”‚  â””â”€ templates/            # HTML
â”œâ”€ notebooks/               # sanity checks, training report
â”œâ”€ src/
â”‚  â”œâ”€ core/                 # tensor utils, inits, losses, optim, metrics
â”‚  â”œâ”€ layers/               # conv, pool, dense, activations, bn, dropout
â”‚  â”œâ”€ models/               # sequential, small presets
â”‚  â”œâ”€ data/                 # mnist, cifar10
â”‚  â””â”€ train/                # loop, callbacks, scheduler, logger, cli
â”œâ”€ tests/                   # unit and numeric gradient checks
â””â”€ reports/                 # figures and CSV logs
```

## Math overview

* **Convolution** via `im2col` trick to reduce conv to GEMM.
  Shapes: input `(N, C_in, H, W)`, weights `(C_out, C_in, KH, KW)`, output `(N, C_out, H_out, W_out)`.
* **Backward Conv**: compute `dW` from cached columns and `dX` via `col2im`.
* **Pooling**: max pool with index mask for backward, average pool with uniform gradient.
* **Softmax Cross Entropy**: stable softmax with logits shift. Derive analytic gradient `dL/dlogits = p - y_onehot`.
* **BatchNorm2D**: per channel mean and variance, running stats in eval mode, full backward.

References are cited in code comments and notebooks.

## Reproducibility

* Seed all RNGs and log them.
* Save configs, metrics, and checkpoints.
* Use `reports/results.csv` for runs. Figures go to `reports/figures/`.

## Testing

Run all tests:

```bash
pytest -q
```

Key tests check:

* `im2col` and `col2im` inverses on valid windows.
* `Conv2D.backward` vs numerical gradients.
* Pooling backward correctness.
* BatchNorm backward against finite differences.
* Loss gradients for MSE and softmax CE.

## Configs

Example `src/configs/mnist_lenet.yaml`:

```yaml
dataset: mnist
model: lenet_mnist
seed: 42
train:
  epochs: 15
  batch_size: 128
  lr: 0.01
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  scheduler:
    name: step
    step_size: 5
    gamma: 0.5
callbacks:
  - early_stopping:
      monitor: val_acc
      patience: 3
  - checkpoint:
      filepath: "checkpoints/lenet_mnist_best.npz"
      monitor: val_acc
      mode: max
```

## Milestones

### Core CNN (Completed)
* [âœ…] `im2col` and `col2im` with tests
* [âœ…] `Conv2D` forward and backward
* [âœ…] `MaxPool2D` forward and backward
* [âœ…] `Dense`, `ReLU`, `SoftmaxCE` with gradients
* [âœ…] Train LeNet on MNIST to >98% val acc
* [âœ…] BatchNorm2D + Dropout
* [âœ…] Tiny VGG on CIFAR-10 to a reasonable baseline

### DermaScan (New Project)
* [âœ…] Project structure and architecture
* [âœ…] FastAPI backend with REST endpoints
* [âœ…] Image preprocessing pipeline
* [âœ…] CNN model architecture for dermatology
* [âœ…] Medical conditions database (7 classes)
* [âœ…] Web frontend (HTML/CSS/JS)
* [âœ…] Complete documentation
* [â³] HAM10000 data loader
* [â³] Model training pipeline
* [â³] Production deployment

## Limitations

* Educational focus, not optimized for speed.
* No mixed precision, no GPU, no autograd.
* Only small models and datasets.

## License

MIT

