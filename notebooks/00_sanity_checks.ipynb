{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f908dbd",
   "metadata": {},
   "source": [
    "# Sanity Checks — CNN-from-scratch\n",
    "\n",
    "\n",
    "### What this notebook does\n",
    "1. Verifies imports and environment.\n",
    "2. Runs `im2col/col2im` adjoint check.\n",
    "3. Builds a small model (LeNet for MNIST) and runs a forward pass.\n",
    "4. Runs a single optimizer step and checks loss improvement.\n",
    "5. Tries to overfit a tiny subset for a few iterations (loss should go down fast).\n",
    "6. Prints gradient norms to catch NaNs or explosions.\n",
    "7. Optionally saves a checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78339f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:42:04) [MSC v.1943 64 bit (AMD64)]\n",
      "Platform: Windows-10-10.0.26100-SP0\n",
      "CWD: c:\\Users\\arnov\\Desktop\\CNN-from-scratch\\notebooks\n",
      "Sys.path[0]: c:\\Users\\arnov\\miniconda3\\envs\\cnn-from-scratch\\python310.zip\n",
      "Project root assumed: c:\\Users\\arnov\\Desktop\\CNN-from-scratch\n",
      "Has src/? True\n"
     ]
    }
   ],
   "source": [
    "import sys, os, platform, numpy as np\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "print('CWD:', os.getcwd())\n",
    "print('Sys.path[0]:', sys.path[0])\n",
    "\n",
    "# Ensure project root (folder containing `src/`) is on sys.path\n",
    "ROOT = os.path.abspath(os.path.join(os.getcwd()))\n",
    "if not os.path.isdir(os.path.join(ROOT, 'src')):\n",
    "    # If the notebook sits in a subfolder, try one level up\n",
    "    ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if os.path.isdir(os.path.join(ROOT, 'src')) and ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "print('Project root assumed:', ROOT)\n",
    "print('Has src/?', os.path.isdir(os.path.join(ROOT, 'src')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e28e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project imports OK\n"
     ]
    }
   ],
   "source": [
    "# Core imports from your project\n",
    "from src.core.utils import im2col, col2im, one_hot, make_batches, set_seed\n",
    "from src.core.losses import softmax_cross_entropy, softmax_cross_entropy_backward\n",
    "from src.core.metrics import accuracy\n",
    "from src.core.optim import Adam\n",
    "from src.layers.conv2d import Conv2D\n",
    "from src.layers.dense import Dense\n",
    "from src.layers.activations import ReLU\n",
    "from src.layers.pooling import MaxPool2D\n",
    "from src.layers.batchnorm import BatchNorm2D\n",
    "from src.layers.dropout import Dropout\n",
    "from src.models.convnet_small import lenet_mnist\n",
    "from src.models.sequential import Sequential\n",
    "from src.data.mnist import load_mnist\n",
    "print('Project imports OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229c8b9",
   "metadata": {},
   "source": [
    "## 1) `im2col/col2im` adjoint check\n",
    "We verify that `<im2col(x), C> == <x, col2im(C)>` which should hold for any shapes and kernel/stride/pad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfe20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjoint diff: 1.4210854715202004e-14\n",
      "✅ im2col/col2im adjoint OK\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "x = np.random.randn(2, 3, 5, 5).astype(np.float64)\n",
    "KH, KW, stride, pad = 3, 3, 1, 1\n",
    "cols = im2col(x, (KH, KW), stride=stride, pad=pad).astype(np.float64)\n",
    "C = np.random.randn(*cols.shape).astype(np.float64)\n",
    "lhs = np.sum(cols * C)\n",
    "rhs = np.sum(x * col2im(C, x.shape, (KH, KW), stride=stride, pad=pad))\n",
    "print('Adjoint diff:', float(abs(lhs - rhs)))\n",
    "assert np.allclose(lhs, rhs, rtol=1e-10, atol=1e-10)\n",
    "print('im2col/col2im adjoint OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef9481",
   "metadata": {},
   "source": [
    "## 2) Build LeNet for MNIST and forward pass\n",
    "We create the model in float32 for speed, run a forward pass on random inputs, and check shapes and finite values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (8, 10)\n",
      "Finite check: True\n",
      "✅ Forward pass OK\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "set_seed(42)\n",
    "model = lenet_mnist(num_classes=10)\n",
    "# switch model to training\n",
    "model.train()\n",
    "dummy = np.random.randn(8, 1, 28, 28).astype(np.float32)\n",
    "logits = model.forward(dummy, training=True)\n",
    "print('Logits shape:', logits.shape)\n",
    "print('Finite check:', np.isfinite(logits).all())\n",
    "assert logits.shape == (8, 10)\n",
    "assert np.isfinite(logits).all()\n",
    "print('Forward pass OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e0fd0",
   "metadata": {},
   "source": [
    "## 3) Single optimizer step on random labels\n",
    "We compute CE loss against random labels and do a single Adam step. Loss should not be NaN and typically decreases a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: 4.136271525692004 after one step: 5.437370820656147\n",
      "Grad norm dX: 0.9990653157659999\n",
      "✅ Single step OK\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randint(0, 10, size=(8,))\n",
    "y1 = one_hot(y, 10)\n",
    "loss0 = softmax_cross_entropy(logits, y1)\n",
    "grad_logits = softmax_cross_entropy_backward(logits, y1)\n",
    "dx = model.backward(grad_logits)\n",
    "opt = Adam(lr=1e-3)\n",
    "opt.step(model.params(), model.grads())\n",
    "logits1 = model.forward(dummy, training=True)\n",
    "loss1 = softmax_cross_entropy(logits1, y1)\n",
    "print('Loss before:', float(loss0), 'after one step:', float(loss1))\n",
    "print('Grad norm dX:', float(np.linalg.norm(dx)))\n",
    "assert np.isfinite(loss0) and np.isfinite(loss1)\n",
    "print('Single step OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3f5c7",
   "metadata": {},
   "source": [
    "## 4) Tiny overfit on MNIST subset (optional)\n",
    "We try to overfit a tiny subset of MNIST (e.g., 256 samples) for a few epochs. Loss should go down quickly.\n",
    "\n",
    "**Note:** This cell will download MNIST into `data/mnist/`. If your network blocks download, copy the files manually first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48b3ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=2.4667 acc=0.141\n",
      "epoch 2: loss=2.2397 acc=0.188\n",
      "epoch 3: loss=2.1194 acc=0.227\n",
      "epoch 4: loss=2.0754 acc=0.246\n",
      "epoch 5: loss=2.0133 acc=0.289\n",
      "✅ Tiny overfit completed (loss should trend down)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    (Xtr, ytr), (Xval, yval), (_, _), num_classes = load_mnist()\n",
    "    # take a tiny subset\n",
    "    n_small = 256\n",
    "    Xs, ys = Xtr[:n_small].astype(np.float32), ytr[:n_small]\n",
    "    model = lenet_mnist(num_classes=num_classes)\n",
    "    model.train()\n",
    "    opt = Adam(lr=1e-3)\n",
    "    for epoch in range(1, 6):\n",
    "        # simple SGD loop over whole tiny set in one batch\n",
    "        logits = model.forward(Xs, training=True)\n",
    "        y1 = one_hot(ys, num_classes)\n",
    "        loss = softmax_cross_entropy(logits, y1)\n",
    "        grad = softmax_cross_entropy_backward(logits, y1)\n",
    "        _ = model.backward(grad)\n",
    "        opt.step(model.params(), model.grads())\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        acc = (preds == ys).mean()\n",
    "        print(f\"epoch {epoch}: loss={loss:.4f} acc={acc:.3f}\")\n",
    "    print(' Tiny overfit completed (loss should trend down)')\n",
    "except Exception as e:\n",
    "    print('MNIST tiny overfit skipped due to error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251164f7",
   "metadata": {},
   "source": [
    "## 5) Gradient norm diagnostics\n",
    "Print parameter and gradient norms to spot NaNs or exploding/vanishing gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae38f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.56899767290372\n",
      "0.Conv2D.W                               | param_norm=3.333e+00 | grad_norm=3.135e+00\n",
      "0.Conv2D.b                               | param_norm=0.000e+00 | grad_norm=1.483e+00\n",
      "3.Conv2D.W                               | param_norm=5.664e+00 | grad_norm=1.487e+01\n",
      "3.Conv2D.b                               | param_norm=0.000e+00 | grad_norm=8.505e-01\n",
      "6.Dense.W                                | param_norm=1.547e+01 | grad_norm=2.094e+01\n",
      "6.Dense.b                                | param_norm=0.000e+00 | grad_norm=4.924e-01\n",
      "8.Dense.W                                | param_norm=1.313e+01 | grad_norm=9.745e+00\n",
      "8.Dense.b                                | param_norm=0.000e+00 | grad_norm=4.216e-01\n",
      "11.Dense.W                               | param_norm=4.446e+00 | grad_norm=8.279e+00\n",
      "11.Dense.b                               | param_norm=0.000e+00 | grad_norm=3.502e-01\n",
      "✅ Gradient norms printed\n"
     ]
    }
   ],
   "source": [
    "model = lenet_mnist(num_classes=10)\n",
    "model.train()\n",
    "xb = np.random.randn(32, 1, 28, 28).astype(np.float32)\n",
    "yb = np.random.randint(0, 10, size=(32,))\n",
    "logits = model.forward(xb, training=True)\n",
    "y1 = one_hot(yb, 10)\n",
    "loss = softmax_cross_entropy(logits, y1)\n",
    "grad = softmax_cross_entropy_backward(logits, y1)\n",
    "_ = model.backward(grad)\n",
    "\n",
    "print('Loss:', float(loss))\n",
    "for k, v in model.params().items():\n",
    "    g = model.grads().get(k)\n",
    "    print(f\"{k:40s} | param_norm={np.linalg.norm(v):.3e} | grad_norm={np.linalg.norm(g) if g is not None else float('nan'):.3e}\")\n",
    "print('Gradient norms printed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444f75d",
   "metadata": {},
   "source": [
    "## 6) Optional: save a quick checkpoint\n",
    "We save all current parameters to `checkpoints/sanity_check.npz` for later debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c4101e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to checkpoints/sanity_check.npz\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "np.savez('checkpoints/sanity_check.npz', **model.params())\n",
    "print('Saved to checkpoints/sanity_check.npz')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
